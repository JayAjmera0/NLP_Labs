{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize NLTK's WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Combine stopwords from NLTK and scikit-learn\n",
    "stop_words = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS))\n",
    "\n",
    "# Set of punctuation characters\n",
    "punctuation = string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame\n",
    "df = pd.read_csv('./imdb.csv')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and word not in punctuation]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    # Return the preprocessed text\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply preprocessing to each review and add as a new column\n",
    "df['preprocessed_review'] = df['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove_path = 'glove.6B.100d.txt'  # Example for 100-dimensional embeddings\n",
    "embeddings_index = {}\n",
    "\n",
    "# Load GloVe vectors into a dictionary\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_review_embedding(review):\n",
    "    # Tokenize the preprocessed review\n",
    "    words = review.split()\n",
    "    # Get the word vectors for each word in the review\n",
    "    vectors = [embeddings_index.get(word) for word in words if word in embeddings_index]\n",
    "    if not vectors:  # Handle the case where no words are in the embeddings\n",
    "        return np.zeros(100)  # Assuming 100-dimensional embeddings\n",
    "    # Average the word vectors to get a single vector for the review\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply the function to get embeddings for each preprocessed review\n",
    "df['embedding'] = df['preprocessed_review'].apply(get_review_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map sentiment labels to integers\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "# Convert embeddings and labels to numpy arrays\n",
    "X = np.stack(df['embedding'].values)\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)  # Initial hidden state\n",
    "        out, _ = self.rnn(x.unsqueeze(1), h0)  # RNN output\n",
    "        out = self.fc(out[:, -1, :])  # Fully connected layer on last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2519\n",
      "Epoch [2/10], Loss: 0.0974\n",
      "Epoch [3/10], Loss: 0.0836\n",
      "Epoch [4/10], Loss: 0.0700\n",
      "Epoch [5/10], Loss: 1.1127\n",
      "Epoch [6/10], Loss: 0.7555\n",
      "Epoch [7/10], Loss: 1.2477\n",
      "Epoch [8/10], Loss: 0.3590\n",
      "Epoch [9/10], Loss: 0.6278\n",
      "Epoch [10/10], Loss: 1.5655\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_size = 100  # GloVe embedding size\n",
    "hidden_size = 50  # RNN hidden size\n",
    "output_size = 2   # Number of output classes (positive, negative)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.95%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple LSTM model\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  # Hidden state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  # Cell state\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x.unsqueeze(1), (h0, c0))  # x needs to be (batch_size, seq_length, input_size)\n",
    "        out = self.fc(out[:, -1, :])  # Fully connected layer on last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_size = 100  # GloVe embedding size\n",
    "hidden_size = 50  # LSTM hidden size\n",
    "output_size = 2   # Number of output classes (positive, negative)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "num_layers = 1    # Number of LSTM layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3121\n",
      "Epoch [2/10], Loss: 1.6306\n",
      "Epoch [3/10], Loss: 0.3432\n",
      "Epoch [4/10], Loss: 0.1876\n",
      "Epoch [5/10], Loss: 0.1946\n",
      "Epoch [6/10], Loss: 0.2773\n",
      "Epoch [7/10], Loss: 0.4476\n",
      "Epoch [8/10], Loss: 0.4603\n",
      "Epoch [9/10], Loss: 0.3009\n",
      "Epoch [10/10], Loss: 0.2425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleLSTM(input_size, hidden_size, output_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.80%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset of data\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize, remove punctuation, lowercase, remove stopwords, and lemmatize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the 'review' column\n",
    "df['processed'] = df['review'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building vocabulary and encoding sentences as indices\n",
    "vocab = {}\n",
    "word_count = 1\n",
    "encoded_reviews = []\n",
    "\n",
    "for review in df['processed']:\n",
    "    encoded_review = []\n",
    "    for word in review:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = word_count\n",
    "            word_count += 1\n",
    "        encoded_review.append(vocab[word])\n",
    "    encoded_reviews.append(encoded_review)\n",
    "\n",
    "# Padding sequences to ensure consistent length\n",
    "max_length = max(len(review) for review in encoded_reviews)\n",
    "encoded_reviews = [review + [0] * (max_length - len(review)) for review in encoded_reviews]\n",
    "#Convert sentiment labels to integers\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert data to tensors\n",
    "X = torch.tensor(encoded_reviews, dtype=torch.long)\n",
    "y = torch.tensor(df['sentiment'].values, dtype=torch.long)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on-the-fly embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define LSTM model with on-the-fly embedding\n",
    "class LSTMWithEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  # Hidden state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  # Cell state\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Use the last output of LSTM\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab) + 1  # Add 1 for padding index\n",
    "embedding_dim = 100\n",
    "hidden_size = 50\n",
    "output_size = 2  # Binary classification (positive/negative)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "num_layers = 1\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTMWithEmbedding(vocab_size, embedding_dim, hidden_size, output_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jay\n",
      "[nltk_data]     Ajmera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jay\n",
      "[nltk_data]     Ajmera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jay\n",
      "[nltk_data]     Ajmera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6933\n",
      "Epoch [2/10], Loss: 0.6931\n",
      "Epoch [3/10], Loss: 0.7112\n",
      "Epoch [4/10], Loss: 0.7804\n",
      "Epoch [5/10], Loss: 0.6932\n",
      "Epoch [6/10], Loss: 0.7200\n",
      "Epoch [7/10], Loss: 0.6874\n",
      "Epoch [8/10], Loss: 0.6957\n",
      "Epoch [9/10], Loss: 0.6934\n",
      "Epoch [10/10], Loss: 0.6482\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.40%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1):\n",
    "        super(RNNWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  # Hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Use the last output of RNN\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 0.7103\n",
      "Epoch [2/4], Loss: 0.8658\n",
      "Epoch [3/4], Loss: 0.7123\n",
      "Epoch [4/4], Loss: 1.1261\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab) + 1  # Add 1 for padding index\n",
    "embedding_dim = 100\n",
    "hidden_size = 50\n",
    "output_size = 2  # Binary classification (positive/negative)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 4\n",
    "num_layers = 1\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = RNNWithEmbedding(vocab_size, embedding_dim, hidden_size, output_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.40%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test Accuracy of Different Models')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plot all accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['SimpleRNN', 'SimpleLSTM', 'LSTMWithEmbedding', 'RNNWithEmbedding'], [76.95,78, 49,50])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy of Different Models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msa_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
